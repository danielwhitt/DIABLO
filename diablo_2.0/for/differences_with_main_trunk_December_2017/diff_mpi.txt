183,321d182
< !
<       SUBROUTINE GHOST_VL2_MPI
< ! This subroutine is part of the MPI package for the VL2 advection scheme 
< ! we need to share TH(NY-1) points with the grid above (J=0) points 
< ! and we need to share (TH(3)) points with the grid below (J=NY+2)
< ! in this case
<       INCLUDE 'header'
< 
<       INTEGER I,J,K,N
< 
< ! Define the arrays that will be used for data packing.  This makes the
< ! communication between processes more efficient by only requiring one
< ! send and recieve.
< ! The size of the buffer TH array is 0:NXM,0:NZP-1,N_TH
<       REAL*8 OCPACK(0:NXM,0:NZP-1,N_TH)
<       REAL*8 ICPACK(0:NXM,0:NZP-1,N_TH)
< 
< ! If we are using more than one processor, then we need to pass data
< 
<       IF (NPROCY.gt.1) THEN
< 
< ! First, Pass data up the chain to higher ranked processes
< 
<       IF (RANKY.eq.0) THEN
< ! If we are the lowest ranked process, then we don't need to recieve
< ! data at the lower ghost cells.
< 
< ! Pass data up to the next process from NY-2
<       DO N=1,N_TH 
<         DO K=0,NZP-1
<           DO I=0,NXM
<             OCPACK(I,K,N)=TH(I,K,NY-1,N)
<           END DO
<         END DO
<       END DO
< ! Now, we have packed the data into a compact array, pass the data up
<         CALL MPI_SEND(OCPACK,(N_TH)*(NXM+1)*(NZP)
<      &               ,MPI_DOUBLE_PRECISION
<      &               ,RANKY+1,37,MPI_COMM_Y,IERROR)
< 
< ! End if RANK=0
<       ELSE IF (RANKY.LT.NPROCY-1) THEN
< ! Here, we are one of the middle processes and we need to pass data
< ! up and recieve data from below
<       DO N=1,N_TH
<         DO K=0,NZP-1
<           DO I=0,NXM
<             OCPACK(I,K,N)=TH(I,K,NY-1,N)
<           END DO
<         END DO
<       END DO
< 
< ! Use MPI_SENDRECV since we need to recieve and send data
<         CALL MPI_SEND(OCPACK,(N_TH)*(NXM+1)*(NZP)
<      &               ,MPI_DOUBLE_PRECISION
<      &               ,RANKY+1,37,MPI_COMM_Y,IERROR)
< 
<         CALL MPI_RECV(ICPACK,(N_TH)*(NXM+1)*(NZP)
<      &               ,MPI_DOUBLE_PRECISION
<      &               ,RANKY-1,37,MPI_COMM_Y,STATUS,IERROR)
< ! Now, unpack the data that we have recieved
<       DO N=1,N_TH
<         DO K=0,NZP-1
<           DO I=0,NXM
<             TH_M1(I,K,N)=ICPACK(I,K,N)
<           END DO
<         END DO
<       END DO
< 
<       ELSE
< ! Otherwise, we must be the uppermost process with RANK=NPROCY-1
< ! Here, we need to recieve data from below, but don't need to send data up
<         CALL MPI_RECV(ICPACK,(N_TH)*(NXM+1)*(NZP)
<      &               ,MPI_DOUBLE_PRECISION
<      &               ,RANKY-1,37,MPI_COMM_Y,STATUS,IERROR)
< ! Unpack the data that we have recieved
<       DO N=1,N_TH
<         DO K=0,NZP-1
<           DO I=0,NXM
<             TH_M1(I,K,N)=ICPACK(I,K,N)
<           END DO
<         END DO
<       END DO
<       END IF
< 
< ! Now, send data down the chain
<       IF (RANKY.EQ.(NPROCY-1)) THEN
<       DO N=1,N_TH
<         DO K=0,NZP-1
<           DO I=0,NXM
<             OCPACK(I,K,N)=TH(I,K,3,N)
<           END DO
<         END DO
<       END DO
< ! Now, we have packed the data into a compact array, pass the data up
<         CALL MPI_SEND(OCPACK,(N_TH)*(NXM+1)*(NZP)
<      &               ,MPI_DOUBLE_PRECISION
<      &               ,RANKY-1,38,MPI_COMM_Y,IERROR)
<       ELSE IF (RANKY.GT.0) THEN
< ! Here, we are one of the middle processes and we need to pass data
< ! down and recieve data from above us
<       DO N=1,N_TH
<         DO K=0,NZP-1
<           DO I=0,NXM
<             OCPACK(I,K,N)=TH(I,K,3,N)
<           END DO
<         END DO
<       END DO
< 
<         CALL MPI_SEND(OCPACK,(N_TH)*(NXM+1)*(NZP)
<      &               ,MPI_DOUBLE_PRECISION
<      &               ,RANKY-1,38,MPI_COMM_Y,IERROR)
< 
<         CALL MPI_RECV(ICPACK,(N_TH)*(NXM+1)*(NZP)
<      &               ,MPI_DOUBLE_PRECISION
<      &               ,RANKY+1,38,MPI_COMM_Y,STATUS,IERROR)
< ! Now, unpack the data that we have recieved
<       DO N=1,N_TH
<         DO K=0,NZP-1
<           DO I=0,NXM
<             TH_P1(I,K,N)=ICPACK(I,K,N)
<           END DO
<         END DO
<       END DO
<       ELSE
< ! Here, we must be the lowest process (RANK=0) and we need to recieve
< ! data from above
<         CALL MPI_RECV(ICPACK,(N_TH)*(NXM+1)*(NZP)
<      &               ,MPI_DOUBLE_PRECISION
<      &               ,RANKY+1,38,MPI_COMM_Y,STATUS,IERROR)
< ! Unpack the data that we have recieved
<       DO N=1,N_TH
<         DO K=0,NZP-1
<           DO I=0,NXM
<             TH_P1(I,K,N)=ICPACK(I,K,N)
<           END DO
<         END DO
<       END DO
<       END IF
323,326d183
<       END IF 
< 
<       RETURN
<       END
343,344c200,201
<       REAL*8 OCPACK(0:NXM,0:NZP-1,1+N_TH)
<       REAL*8 ICPACK(0:NXM,0:NZP-1,1+N_TH)
---
>       REAL*8 OCPACK(0:NXM,0:NZP-1)
>       REAL*8 ICPACK(0:NXM,0:NZP-1)
355,365d211
< ! ---
< ! DAN changed NU_T here (other wise should be zero)
< ! This is for the channel configuration with the bottom
< ! viewed as 'open' rather than a wall per se
<         DO K=0,NZP-1
<           DO I=0,NXM
<             NU_T(I,K,1)=NU_T(I,K,3)
<             NU_T(I,K,2)=NU_T(I,K,3)
<           END DO
<         END DO
<         DO N=1,N_TH
368,369c214,215
<             PRM1_T(I,K,1,N)=PRM1_T(I,K,3,N)
<             PRM1_T(I,K,2,N)=PRM1_T(I,K,3,N)
---
>             NU_T(I,K,1)=0.d0
>             NU_T(I,K,2)=0.d0
372c218
<         END DO
---
> 
376,382c222
<             OCPACK(I,K,1)=NU_T(I,K,NY)
<           END DO
<         END DO
<         DO N=1,N_TH
<         DO K=0,NZP-1
<           DO I=0,NXM
<             OCPACK(I,K,1+N)=PRM1_T(I,K,NY,N)
---
>             OCPACK(I,K)=NU_T(I,K,NY)
385d224
<         END DO
387c226
<         CALL MPI_SEND(OCPACK,(1+N_TH)*(NXM+1)*(NZP)
---
>         CALL MPI_SEND(OCPACK,(NXM+1)*(NZP)
397,403c236
<             OCPACK(I,K,1)=NU_T(I,K,NY)
<           END DO
<         END DO
<         DO N=1,N_TH
<         DO K=0,NZP-1
<           DO I=0,NXM
<             OCPACK(I,K,1+N)=PRM1_T(I,K,NY,N)
---
>             OCPACK(I,K)=NU_T(I,K,NY)
406d238
<         END DO
408c240
<         CALL MPI_SEND(OCPACK,(1+N_TH)*(NXM+1)*(NZP)
---
>         CALL MPI_SEND(OCPACK,(NXM+1)*(NZP)
412c244
<         CALL MPI_RECV(ICPACK,(1+N_TH)*(NXM+1)*(NZP)
---
>         CALL MPI_RECV(ICPACK,(NXM+1)*(NZP)
418c250
<             NU_T(I,K,1)=ICPACK(I,K,1)
---
>             NU_T(I,K,1)=ICPACK(I,K)
421,427c253
<         DO N=1,N_TH
<         DO K=0,NZP-1
<           DO I=0,NXM
<             PRM1_T(I,K,1,N)=ICPACK(I,K,1+N)
<           END DO
<         END DO
<         END DO
---
> 
431c257
<         CALL MPI_RECV(ICPACK,(1+N_TH)*(NXM+1)*(NZP)
---
>         CALL MPI_RECV(ICPACK,(NXM+1)*(NZP)
437,443c263
<             NU_T(I,K,1)=ICPACK(I,K,1)
<           END DO
<         END DO
<         DO N=1,N_TH
<         DO K=0,NZP-1
<           DO I=0,NXM
<             PRM1_T(I,K,1,N)=ICPACK(I,K,1+N)
---
>             NU_T(I,K,1)=ICPACK(I,K)
446d265
<         END DO
461,468d279
<         DO N=1,N_TH
<         DO K=0,NZP-1
<           DO I=0,NXM
<             PRM1_T(I,K,NY,N)=1.d0
<             PRM1_T(I,K,NY+1,N)=1.d0
<           END DO
<         END DO
<         END DO
473c284
<             OCPACK(I,K,1)=NU_T(I,K,2)
---
>             OCPACK(I,K)=NU_T(I,K,2)
476,482d286
<         DO N=1,N_TH
<         DO K=0,NZP-1
<           DO I=0,NXM
<             OCPACK(I,K,1+N)=PRM1_T(I,K,2,N)
<           END DO
<         END DO
<         END DO
484c288
<         CALL MPI_SEND(OCPACK,(1+N_TH)*(NXM+1)*(NZP)
---
>         CALL MPI_SEND(OCPACK,(NXM+1)*(NZP)
492c296
<             OCPACK(I,K,1)=NU_T(I,K,2)
---
>             OCPACK(I,K)=NU_T(I,K,2)
495,501d298
<         DO N=1,N_TH
<         DO K=0,NZP-1
<           DO I=0,NXM
<             OCPACK(I,K,1+N)=PRM1_T(I,K,2,N)
<           END DO
<         END DO
<         END DO
503c300
<         CALL MPI_SEND(OCPACK,(1+N_TH)*(NXM+1)*(NZP)
---
>         CALL MPI_SEND(OCPACK,(NXM+1)*(NZP)
507c304
<         CALL MPI_RECV(ICPACK,(1+N_TH)*(NXM+1)*(NZP)
---
>         CALL MPI_RECV(ICPACK,(NXM+1)*(NZP)
513,519c310
<             NU_T(I,K,NY+1)=ICPACK(I,K,1)
<           END DO
<         END DO
<         DO N=1,N_TH
<         DO K=0,NZP-1
<           DO I=0,NXM
<             PRM1_T(I,K,NY+1,N)=ICPACK(I,K,1+N)
---
>             NU_T(I,K,NY+1)=ICPACK(I,K)
522d312
<         END DO
526c316
<         CALL MPI_RECV(ICPACK,(1+N_TH)*(NXM+1)*(NZP)
---
>         CALL MPI_RECV(ICPACK,(NXM+1)*(NZP)
532c322
<             NU_T(I,K,NY+1)=ICPACK(I,K,1)
---
>             NU_T(I,K,NY+1)=ICPACK(I,K)
535,541d324
<         DO N=1,N_TH
<         DO K=0,NZP-1
<           DO I=0,NXM
<             PRM1_T(I,K,NY+1,N)=ICPACK(I,K,1+N)
<           END DO
<         END DO
<         END DO
549,553c332,333
< ! DAN changed NU_T here at the bottom two grid cells (other wise should be zero)
< ! This is for the channel configuration with the bottom
< ! viewed as 'open' rather than a wall per se
<             NU_T(I,K,1)=NU_T(I,K,3)
<             NU_T(I,K,2)=NU_T(I,K,3)
---
>             NU_T(I,K,1)=0.d0
>             NU_T(I,K,2)=0.d0
558,567c338
<         DO N=1,N_TH
<         DO K=0,NZP-1
<           DO I=0,NXM
<             PRM1_T(I,K,1,N)=PRM1_T(I,K,3,N)
<             PRM1_T(I,K,2,N)=PRM1_T(I,K,3,N)
<             PRM1_T(I,K,NY,N)=1.d0
<             PRM1_T(I,K,NY+1,N)=1.d0
<           END DO
<         END DO
<         END DO
---
> 
1346,1350c1117
<       INTEGER I,K,KD83
< ! set to KD81=0 to impose zero vertical pressure gradient BC at the bottom boundary
< ! set to KD81=1 to impose Klemp & Durran 83 vertical pressure gradient BC at the bottom boundary
< ! WARNING KD83=1 is not tested
<         KD83=0
---
>       INTEGER I,K
1369d1135
<           IF (KD83.EQ.0) THEN
1371,1379d1136
<           ELSE
< ! dphi/dz = N*dw_k/dz/(Kx^2 + Kz^2)**0.5
<             VEC_C(I,1)=((TH_BC_YMIN_C1(1))**0.5d0)/                     &
<      &                  ((KX2(I)+KZ2(K))**0.5d0)*                       &
<      &                  (CU2(I,K,3))
<           IF (ABS(CU2(I,K,2)).GT.1.0d-10) THEN
<              WRITE(*,*) 'ERROR U2 KD83'
<           END IF
<           END IF
1448a1206
> 
1464a1223
>  
